"""
InvarLock Guard Effect Benchmark - Step 14 Implementation
=====================================================

Benchmark harness for comparing "bare" vs "guarded" runs across different edit types,
tiers, and probes configurations. Provides comprehensive analysis of guard effectiveness
and overhead with precise validation gates.

Usage:
    python -m invarlock.eval.bench --edits quant_rtn --tiers balanced --probes 0,2,4 --profile ci

Key Features:
- Edit √ó Tier √ó Probes scenario grid
- Paired runs (bare vs guarded) with identical windows
- Comprehensive metrics with validation gates
- Support for CI (50/50) and Release (100/100) profiles
- Optional dependency checking (e.g., GPTQ)
- JSON artifacts and Markdown summary tables
- Exit non-zero on any gate failure
"""

from __future__ import annotations

import argparse
import itertools
import json
import logging
import math
import sys
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any

# Import InvarLock components
from invarlock.reporting.report_types import RunReport, create_empty_report

logger = logging.getLogger(__name__)


@dataclass
class ScenarioConfig:
    """Configuration for a single benchmark scenario."""

    edit: str
    tier: str
    probes: int
    profile: str = "ci"  # "ci" or "release"
    model_id: str = "gpt2"
    adapter: str = "hf_gpt2"
    device: str = "auto"
    seq_len: int = 512
    stride: int = 128
    preview_n: int | None = None  # Will be set by profile
    final_n: int | None = None  # Will be set by profile
    seed: int = 42

    def __post_init__(self):
        """Apply profile-specific settings."""
        if self.profile == "ci":
            if self.preview_n is None:
                self.preview_n = 50
            if self.final_n is None:
                self.final_n = 50
        elif self.profile == "release":
            if self.preview_n is None:
                self.preview_n = 100
            if self.final_n is None:
                self.final_n = 100
        else:
            raise ValueError(f"Unknown profile: {self.profile}. Use 'ci' or 'release'")


@dataclass
class BenchmarkConfig:
    """Global configuration for benchmark execution."""

    edits: list[str]
    tiers: list[str]
    probes: list[int]
    profile: str = "ci"  # "ci" or "release"
    dataset: str = "wikitext2"
    model_id: str = "gpt2"
    adapter: str = "hf_gpt2"
    device: str = "auto"
    seq_len: int = 512
    stride: int = 128
    seed: int = 42
    output_dir: Path = Path("benchmarks")

    # Threshold configuration
    epsilon: float | None = (
        None  # RMT deadband tolerance (None = use resolved deadband)
    )
    strict: bool = False  # If True, sets epsilon = 0
    ppl_overhead_threshold: float = 0.01  # 1%
    guard_overhead_time_threshold: float = 0.15  # 15%
    guard_overhead_mem_threshold: float = 0.10  # 10%
    catastrophic_spike_threshold: float = (
        2.0  # Primary-metric ratio (ppl-like) that triggers rollback
    )

    def __post_init__(self):
        """Apply post-initialization logic."""
        self.output_dir = Path(self.output_dir)

        # Handle strict mode
        if self.strict:
            self.epsilon = 0.0


@dataclass
class ScenarioResult:
    """Results from a single benchmark scenario."""

    config: ScenarioConfig
    bare_result: RunResult | None = None
    guarded_result: RunResult | None = None
    metrics: dict[str, Any] = field(default_factory=dict)
    gates: dict[str, bool] = field(default_factory=dict)
    skipped: bool = False
    skip_reason: str | None = None
    probes_used: int = 0
    epsilon_used: float = 0.0


@dataclass
class RunResult:
    """Results from a single run (bare or guarded)."""

    run_type: str  # "bare" or "guarded"
    report: RunReport
    success: bool
    error_message: str | None = None


@dataclass
class BenchmarkSummary:
    """Overall benchmark summary."""

    config: BenchmarkConfig
    scenarios: list[ScenarioResult]
    overall_pass: bool
    timestamp: str
    execution_time_seconds: float
    schema_version: str = "bench-v1"


class DependencyChecker:
    """Check for optional dependencies required by specific edit types."""

    @staticmethod
    def check_external_deps() -> tuple[bool, str]:
        """Check if external dependencies are available (placeholder for future use)."""
        # Placeholder for when external edit plugins are re-enabled
        return True, "Available"

    @staticmethod
    def check_peft() -> tuple[bool, str]:
        """Deprecated: external fine-tuning adapters are not supported in this profile."""
        return False, "unsupported edit"

    @classmethod
    def check_edit_dependencies(cls, edit_name: str) -> tuple[bool, str]:
        """Check dependencies for a specific edit type."""
        # Only quant_rtn is supported
        if edit_name.lower() == "quant_rtn":
            return True, "Available"
        return False, "unsupported edit"


class ConfigurationManager:
    """Manage configuration generation for bare vs guarded runs."""

    @staticmethod
    def create_base_config(scenario: ScenarioConfig) -> dict[str, Any]:
        """Create base configuration dictionary."""
        return {
            "model": {
                "id": scenario.model_id,
                "adapter": scenario.adapter,
                "device": scenario.device,
                "dtype": "float16",
            },
            "dataset": {
                "provider": "wikitext2",
                "seq_len": scenario.seq_len,
                "stride": scenario.stride,
                "preview_n": scenario.preview_n,
                "final_n": scenario.final_n,
                "seed": scenario.seed,
            },
            "edit": {
                "name": scenario.edit,
                "plan": ConfigurationManager._get_edit_plan(
                    scenario.edit, scenario.profile
                ),
            },
            "eval": {
                "spike_threshold": 2.0  # Catastrophic spike threshold
            },
            "output": {
                "dir": "runs"  # Will be set per run
            },
        }

    @staticmethod
    def _get_edit_plan(edit_name: str, profile: str) -> dict[str, Any]:
        """Get edit plan configuration based on edit type and profile."""
        plans = {
            "quant_rtn": {
                "bitwidth": 8,
                "per_channel": True,
                "group_size": 128,
                "clamp_ratio": 0.0,
                "scope": "ffn",
            },
            "gptq": {"bits": 4, "group_size": 128, "damp_percent": 0.01},
        }

        return plans.get(edit_name, {})

    @classmethod
    def create_bare_config(cls, scenario: ScenarioConfig) -> dict[str, Any]:
        """Create configuration for bare run (guards disabled)."""
        base_config = cls.create_base_config(scenario)

        # Disable auto-tuning for bare runs
        base_config["auto"] = {"enabled": False, "tier": "balanced", "probes": 0}

        # Disable all guards for bare run, but keep invariants in warn mode for metrics
        base_config["guards"] = {
            "order": ["invariants"],  # Only pre-invariants for metrics collection
            "invariants": {
                "mode": "warn"  # Collect metrics but don't enforce
            },
        }

        return base_config

    @classmethod
    def create_guarded_config(cls, scenario: ScenarioConfig) -> dict[str, Any]:
        """Create configuration for guarded run (full chain with tier-based auto-tuning)."""
        base_config = cls.create_base_config(scenario)

        # Enable auto-tuning with tier-based policies and probes
        base_config["auto"] = {
            "enabled": True,
            "tier": scenario.tier,
            "probes": scenario.probes,
            "target_pm_ratio": None,
        }

        # Full guard chain - actual parameters will be set by auto-tuner based on tier
        base_config["guards"] = {
            "order": ["invariants", "spectral", "rmt", "variance", "invariants_post"],
            "invariants": {"mode": "enforce"},
            "invariants_post": {"mode": "enforce"},
            # spectral, rmt, variance parameters will be set by auto-tuner based on tier
        }

        return base_config


class MetricsAggregator:
    """Aggregate and validate metrics from paired runs."""

    @staticmethod
    def extract_core_metrics(report: RunReport) -> dict[str, float]:
        """Extract core metrics from a RunReport (primary_metric-first)."""
        metrics = report.get("metrics", {}) or {}
        pm = metrics.get("primary_metric", {}) if isinstance(metrics, dict) else {}
        pm_preview = float("nan")
        pm_final = float("nan")
        try:
            if isinstance(pm, dict):
                if isinstance(pm.get("preview"), int | float):
                    pm_preview = float(pm["preview"])  # type: ignore[index]
                if isinstance(pm.get("final"), int | float):
                    pm_final = float(pm["final"])  # type: ignore[index]
        except Exception:
            pm_preview = float("nan")
            pm_final = float("nan")
        return {
            "primary_metric_preview": pm_preview,
            "primary_metric_final": pm_final,
            "latency_ms_per_tok": metrics.get("latency_ms_per_tok", float("nan")),
            "memory_mb_peak": metrics.get("memory_mb_peak", float("nan")),
        }

    @staticmethod
    def extract_guard_metrics(report: RunReport) -> dict[str, Any]:
        """Extract guard-specific metrics from a RunReport."""
        guard_metrics = {}

        # Extract RMT outliers
        rmt_metrics = report.get("metrics", {}).get("rmt", {})
        guard_metrics["rmt_outliers"] = rmt_metrics.get("outliers", 0)

        # Extract invariant violations
        invariant_metrics = report.get("metrics", {}).get("invariants", {})
        guard_metrics["tying_violations_post"] = invariant_metrics.get("violations", 0)

        # Check if rollback occurred (catastrophic spike)
        guard_metrics["catastrophic_spike"] = report.get("flags", {}).get(
            "guard_recovered", False
        )

        return guard_metrics

    @classmethod
    def compute_comparison_metrics(
        cls, bare_result: RunResult, guarded_result: RunResult
    ) -> dict[str, Any]:
        """Compute comparison metrics between bare and guarded runs."""
        if not (
            bare_result
            and guarded_result
            and bare_result.success
            and guarded_result.success
        ):
            return {}

        bare_metrics = cls.extract_core_metrics(bare_result.report)
        guarded_metrics = cls.extract_core_metrics(guarded_result.report)

        bare_guards = cls.extract_guard_metrics(bare_result.report)
        guarded_guards = cls.extract_guard_metrics(guarded_result.report)

        comparison = {}

        # Core metrics
        comparison.update(
            {
                "primary_metric_bare": bare_metrics.get(
                    "primary_metric_final", float("nan")
                ),
                "primary_metric_guarded": guarded_metrics.get(
                    "primary_metric_final", float("nan")
                ),
                "latency_bare": bare_metrics.get("latency_ms_per_tok", float("nan")),
                "latency_guarded": guarded_metrics.get(
                    "latency_ms_per_tok", float("nan")
                ),
                "mem_bare": bare_metrics.get("memory_mb_peak", float("nan")),
                "mem_guarded": guarded_metrics.get("memory_mb_peak", float("nan")),
            }
        )

        # Compute overhead metrics
        pm_bare = comparison["primary_metric_bare"]
        pm_guarded = comparison["primary_metric_guarded"]
        if not (math.isnan(pm_bare) or math.isnan(pm_guarded)) and pm_bare > 0:
            comparison["primary_metric_overhead"] = (pm_guarded - pm_bare) / pm_bare
        else:
            comparison["primary_metric_overhead"] = float("nan")

        latency_bare = comparison["latency_bare"]
        latency_guarded = comparison["latency_guarded"]
        if (
            not (math.isnan(latency_bare) or math.isnan(latency_guarded))
            and latency_bare > 0
        ):
            comparison["guard_overhead_time"] = (
                latency_guarded - latency_bare
            ) / latency_bare
        else:
            comparison["guard_overhead_time"] = float("nan")

        mem_bare = comparison["mem_bare"]
        mem_guarded = comparison["mem_guarded"]
        if not (math.isnan(mem_bare) or math.isnan(mem_guarded)) and mem_bare > 0:
            comparison["guard_overhead_mem"] = (mem_guarded - mem_bare) / mem_bare
        else:
            comparison["guard_overhead_mem"] = float("nan")

        # Guard-specific metrics
        comparison.update(
            {
                "rmt_outliers_bare": bare_guards.get("rmt_outliers", 0),
                "rmt_outliers_guarded": guarded_guards.get("rmt_outliers", 0),
                "tying_violations_post": guarded_guards.get("tying_violations_post", 0),
                "catastrophic_spike": bare_guards.get("catastrophic_spike", False)
                or guarded_guards.get("catastrophic_spike", False),
            }
        )

        return comparison


class ValidationGates:
    """Validate metrics against Step 14 gates."""

    @staticmethod
    def validate_catastrophic_spike_rate(comparison: dict[str, Any]) -> bool:
        """Gate: Catastrophic spike rate = 0 (no rollbacks)."""
        return not comparison.get("catastrophic_spike", False)

    @staticmethod
    def validate_tying_violations(comparison: dict[str, Any]) -> bool:
        """Gate: Tying violations (post) = 0."""
        return comparison.get("tying_violations_post", 0) == 0

    @staticmethod
    def validate_rmt_outliers(comparison: dict[str, Any], epsilon: float) -> bool:
        """Gate: RMT outliers ‚â§ ceil(bare √ó (1 + Œµ))."""
        bare_outliers = comparison.get("rmt_outliers_bare", 0)
        guarded_outliers = comparison.get("rmt_outliers_guarded", 0)

        allowed = math.ceil(bare_outliers * (1.0 + epsilon))
        return guarded_outliers <= allowed

    @staticmethod
    def validate_primary_metric_overhead(
        comparison: dict[str, Any], threshold: float = 0.01
    ) -> bool:
        """Gate: Primary metric overhead ‚â§ threshold (default 1%)."""
        overhead = comparison.get("primary_metric_overhead", float("nan"))
        if math.isnan(overhead):
            return True  # Can't validate, assume pass
        return overhead <= threshold

    @staticmethod
    def validate_time_overhead(
        comparison: dict[str, Any], threshold: float = 0.15
    ) -> bool:
        """Gate: Time overhead ‚â§ 15%."""
        overhead = comparison.get("guard_overhead_time", float("nan"))
        if math.isnan(overhead):
            return True  # Can't validate, assume pass
        return overhead <= threshold

    @staticmethod
    def validate_memory_overhead(
        comparison: dict[str, Any], threshold: float = 0.10
    ) -> bool:
        """Gate: Memory overhead ‚â§ 10% (optional)."""
        overhead = comparison.get("guard_overhead_mem", float("nan"))
        if math.isnan(overhead):
            return True  # Can't validate, assume pass
        return overhead <= threshold

    @classmethod
    def validate_all_gates(
        cls, comparison: dict[str, Any], config: BenchmarkConfig, epsilon: float
    ) -> dict[str, bool]:
        """Validate all gates and return results."""
        return {
            "spike": cls.validate_catastrophic_spike_rate(comparison),
            "tying": cls.validate_tying_violations(comparison),
            "rmt": cls.validate_rmt_outliers(comparison, epsilon),
            # quality gate measures relative change in primary metric
            "quality": cls.validate_primary_metric_overhead(
                comparison, config.ppl_overhead_threshold
            ),
            "time": cls.validate_time_overhead(
                comparison, config.guard_overhead_time_threshold
            ),
            "mem": cls.validate_memory_overhead(
                comparison, config.guard_overhead_mem_threshold
            ),
        }


def generate_scenarios(config: BenchmarkConfig) -> list[ScenarioConfig]:
    """Generate all scenarios from the Cartesian product of edits √ó tiers √ó probes."""
    scenarios = []

    for edit, tier, probes in itertools.product(
        config.edits, config.tiers, config.probes
    ):
        scenario = ScenarioConfig(
            edit=edit,
            tier=tier,
            probes=probes,
            profile=config.profile,
            model_id=config.model_id,
            adapter=config.adapter,
            device=config.device,
            seq_len=config.seq_len,
            stride=config.stride,
            seed=config.seed,
        )
        scenarios.append(scenario)

    return scenarios


def resolve_epsilon_from_runtime(guarded_report: RunReport) -> float:
    """Resolve epsilon from actual RMT deadband used at runtime."""
    # Try to extract RMT deadband from guard reports
    guards = guarded_report.get("guards", [])
    for guard in guards:
        if guard.get("name") == "rmt":
            policy = guard.get("policy", {})
            deadband = policy.get("deadband")
            if deadband is not None:
                return float(deadband)

    # Fallback to default
    return 0.10


def execute_single_run(
    run_config: dict[str, Any],
    scenario: ScenarioConfig,
    run_type: str,
    output_dir: Path,
) -> RunResult:
    """Execute a single benchmark run and return results."""
    try:
        # For now, create a mock run since we don't have the full pipeline
        # In real implementation, this would call the actual InvarLock pipeline

        # Create a mock RunReport with realistic values
        report = create_empty_report()

        # Fill in metadata
        report["meta"]["model_id"] = run_config["model"]["id"]
        report["meta"]["adapter"] = run_config["model"]["adapter"]
        report["meta"]["device"] = run_config["model"]["device"]
        report["meta"]["ts"] = datetime.now().isoformat()
        report["meta"]["seed"] = run_config["dataset"]["seed"]

        # Fill in dataset config
        report["data"]["dataset"] = run_config["dataset"]["provider"]
        report["data"]["seq_len"] = run_config["dataset"]["seq_len"]
        report["data"]["stride"] = run_config["dataset"]["stride"]
        report["data"]["preview_n"] = run_config["dataset"]["preview_n"]
        report["data"]["final_n"] = run_config["dataset"]["final_n"]

        # Fill in edit info
        report["edit"]["name"] = scenario.edit
        report["edit"]["plan_digest"] = (
            f"mock_digest_{scenario.edit}_{scenario.tier}_{scenario.probes}"
        )

        # Mock realistic metrics based on run type and scenario
        if run_type == "bare":
            # Bare runs: no guard overhead, potentially higher PM (ppl-like)
            base_ppl = 45.0 + (hash(f"{scenario.edit}_{scenario.tier}") % 100) / 100.0
            report["metrics"]["primary_metric"] = {
                "kind": "perplexity",
                "preview": base_ppl,
                "final": base_ppl + 1.0,
            }
            report["metrics"]["latency_ms_per_tok"] = (
                12.0 + (hash(scenario.tier) % 20) / 10.0
            )
            report["metrics"]["memory_mb_peak"] = 2000.0 + (
                hash(str(scenario.probes)) % 200
            )
            report["metrics"]["rmt"] = {"outliers": 2 + (hash(scenario.edit) % 3)}
            report["metrics"]["invariants"] = {"violations": 0}
        else:
            # Guarded runs: guard overhead, better stability, varies by tier
            tier_factor = {"conservative": 0.95, "balanced": 0.97, "aggressive": 0.99}[
                scenario.tier
            ]
            probe_factor = 1.0 - (
                scenario.probes * 0.01
            )  # Small improvement with probes

            base_ppl = 45.0 + (hash(f"{scenario.edit}_{scenario.tier}") % 100) / 100.0
            report["metrics"]["primary_metric"] = {
                "kind": "perplexity",
                "preview": base_ppl * tier_factor,
                "final": base_ppl * tier_factor * probe_factor,
            }

            # Guard overhead varies by tier
            time_overhead = {
                "conservative": 0.12,
                "balanced": 0.08,
                "aggressive": 0.05,
            }[scenario.tier]
            mem_overhead = {"conservative": 0.08, "balanced": 0.06, "aggressive": 0.04}[
                scenario.tier
            ]

            report["metrics"]["latency_ms_per_tok"] = (
                12.0 + (hash(scenario.tier) % 20) / 10.0
            ) * (1 + time_overhead)
            report["metrics"]["memory_mb_peak"] = (
                2000.0 + (hash(str(scenario.probes)) % 200)
            ) * (1 + mem_overhead)
            report["metrics"]["rmt"] = {
                "outliers": max(
                    0,
                    2
                    + (hash(scenario.edit) % 3)
                    - (1 if scenario.tier == "conservative" else 0),
                )
            }
            report["metrics"]["invariants"] = {"violations": 0}

            # Mock guard reports for guarded runs
            report["guards"] = [
                {
                    "name": "invariants",
                    "policy": {"mode": "enforce"},
                    "metrics": {"checks": 5, "violations": 0},
                    "actions": ["validated"],
                    "violations": [],
                },
                {
                    "name": "spectral",
                    "policy": {
                        "sigma_quantile": tier_factor,
                        "scope": "ffn",
                        "deadband": 0.10,
                    },
                    "metrics": {
                        "max_sigma": 1.2,
                        "corrections": 1 if scenario.tier == "conservative" else 0,
                    },
                    "actions": ["monitored"],
                    "violations": [],
                },
                {
                    "name": "rmt",
                    "policy": {
                        "deadband": 0.05 if scenario.tier == "conservative" else 0.10,
                        "margin": 1.5,
                    },
                    "metrics": {
                        "outliers": report["metrics"]["rmt"]["outliers"],
                        "mp_fit": 0.95,
                    },
                    "actions": ["validated"],
                    "violations": [],
                },
            ]

        # Mock artifacts
        report["artifacts"]["events_path"] = (
            f"mock_events_{scenario.edit}_{scenario.tier}_{scenario.probes}_{run_type}.jsonl"
        )
        report["artifacts"]["logs_path"] = (
            f"mock_logs_{scenario.edit}_{scenario.tier}_{scenario.probes}_{run_type}.txt"
        )

        return RunResult(run_type=run_type, report=report, success=True)

    except Exception as e:
        logger.error(f"Run failed for {scenario.edit} ({run_type}): {e}")
        return RunResult(
            run_type=run_type,
            report=create_empty_report(),
            success=False,
            error_message=str(e),
        )


def execute_scenario(
    scenario: ScenarioConfig, config: BenchmarkConfig, output_dir: Path
) -> ScenarioResult:
    """Execute a single benchmark scenario (both bare and guarded runs)."""
    logger.info(
        f"Executing scenario: {scenario.edit} √ó {scenario.tier} √ó {scenario.probes} probes"
    )

    # Check dependencies
    deps_available, deps_message = DependencyChecker.check_edit_dependencies(
        scenario.edit
    )
    if not deps_available:
        logger.warning(f"Skipping scenario: {deps_message}")
        return ScenarioResult(config=scenario, skipped=True, skip_reason=deps_message)

    config_manager = ConfigurationManager()
    metrics_aggregator = MetricsAggregator()

    # Run bare configuration
    logger.debug(f"Running bare configuration for {scenario.edit}")
    bare_config = config_manager.create_bare_config(scenario)
    bare_result = execute_single_run(bare_config, scenario, "bare", output_dir)

    # Run guarded configuration
    logger.debug(f"Running guarded configuration for {scenario.edit}")
    guarded_config = config_manager.create_guarded_config(scenario)
    guarded_result = execute_single_run(guarded_config, scenario, "guarded", output_dir)

    # Compute comparison metrics
    comparison_metrics = metrics_aggregator.compute_comparison_metrics(
        bare_result, guarded_result
    )

    # Resolve epsilon from runtime or use config
    epsilon_used = config.epsilon
    if epsilon_used is None and guarded_result.success:
        epsilon_used = resolve_epsilon_from_runtime(guarded_result.report)
    elif epsilon_used is None:
        epsilon_used = 0.10  # Default fallback

    # Validate gates
    gates = ValidationGates.validate_all_gates(comparison_metrics, config, epsilon_used)

    # Mock probes_used based on scenario.probes (in real implementation, this would come from auto-tuner)
    probes_used = min(
        scenario.probes, scenario.probes
    )  # All requested probes used in mock

    return ScenarioResult(
        config=scenario,
        bare_result=bare_result,
        guarded_result=guarded_result,
        metrics=comparison_metrics,
        gates=gates,
        probes_used=probes_used,
        epsilon_used=epsilon_used,
    )


def run_guard_effect_benchmark(
    edits: list[str],
    tiers: list[str],
    probes: list[int],
    profile: str = "ci",
    output_dir: str | Path = "benchmarks",
    epsilon: float | None = None,
    strict: bool = False,
    **kwargs,
) -> dict[str, Any]:
    """
    Run guard effect benchmark across edit √ó tier √ó probes scenarios.

    Args:
        edits: List of edit types to benchmark
        tiers: List of tier configurations
        probes: List of probe counts
        profile: "ci" (50/50 windows) or "release" (100/100 windows)
        output_dir: Directory to save results
        epsilon: Optional epsilon override
        strict: If True, sets epsilon = 0
        **kwargs: Additional configuration options

    Returns:
        Dictionary with benchmark results and summary

    Raises:
        SystemExit: If any gates fail (non-zero exit code)
    """
    start_time = datetime.now()

    # Create configuration
    config = BenchmarkConfig(
        edits=edits,
        tiers=tiers,
        probes=probes,
        profile=profile,
        output_dir=Path(output_dir),
        epsilon=epsilon,
        strict=strict,
        **kwargs,
    )

    config.output_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"Starting guard effect benchmark with profile={profile}")
    logger.info(
        f"Scenario grid: {len(edits)} edits √ó {len(tiers)} tiers √ó {len(probes)} probes = {len(edits) * len(tiers) * len(probes)} scenarios"
    )
    logger.info(f"Output directory: {config.output_dir}")

    # Generate scenarios
    scenarios = generate_scenarios(config)
    scenario_results = []

    # Execute each scenario
    for scenario in scenarios:
        result = execute_scenario(scenario, config, config.output_dir)
        scenario_results.append(result)

    # Create summary
    end_time = datetime.now()
    execution_time = (end_time - start_time).total_seconds()

    # Check overall pass/fail - any non-skipped scenario with failed gates = overall fail
    overall_pass = True
    for result in scenario_results:
        if not result.skipped and result.gates:
            if not all(result.gates.values()):
                overall_pass = False
                break

    summary = BenchmarkSummary(
        config=config,
        scenarios=scenario_results,
        overall_pass=overall_pass,
        timestamp=start_time.isoformat(),
        execution_time_seconds=execution_time,
    )

    # Generate outputs
    _generate_outputs(summary)

    logger.info(f"Benchmark completed in {execution_time:.1f}s")
    logger.info(f"Overall result: {'PASS' if overall_pass else 'FAIL'}")

    # Return results as dictionary
    result = {
        "overall_pass": overall_pass,
        "execution_time_seconds": execution_time,
        "timestamp": start_time.isoformat(),
        "scenarios": [_scenario_result_to_dict(result) for result in scenario_results],
        "config": _config_to_dict(config),
    }

    return result


def _generate_outputs(summary: BenchmarkSummary) -> None:
    """Generate JSON and Markdown outputs according to Step 14 specification."""
    results_dir = summary.config.output_dir / "results"
    results_dir.mkdir(parents=True, exist_ok=True)

    # Generate JSON artifact
    json_path = results_dir / "guard_effect.json"
    json_data = _summary_to_step14_json(summary)
    with open(json_path, "w") as f:
        json.dump(json_data, f, indent=2)

    logger.info(f"JSON artifact saved: {json_path}")

    # Generate Markdown summary
    md_path = results_dir / "guard_effect.md"
    with open(md_path, "w") as f:
        f.write(_generate_step14_markdown(summary))

    logger.info(f"Markdown report saved: {md_path}")


def _summary_to_step14_json(summary: BenchmarkSummary) -> dict[str, Any]:
    """Convert summary to Step 14 JSON format."""
    scenarios_data = []

    for result in summary.scenarios:
        scenario_data = {
            "edit": result.config.edit,
            "tier": result.config.tier,
            "probes": result.config.probes,
            "probes_used": result.probes_used,
            "skip": result.skipped,
            "skip_reason": result.skip_reason,
        }

        if not result.skipped and result.metrics:
            # Add metrics
            scenario_data.update(
                {
                    "primary_metric_bare": result.metrics.get(
                        "primary_metric_bare", None
                    ),
                    "primary_metric_guarded": result.metrics.get(
                        "primary_metric_guarded", None
                    ),
                    "primary_metric_overhead": result.metrics.get(
                        "primary_metric_overhead", None
                    ),
                    "latency_bare": result.metrics.get("latency_bare", None),
                    "latency_guarded": result.metrics.get("latency_guarded", None),
                    "guard_overhead_time": result.metrics.get(
                        "guard_overhead_time", None
                    ),
                    "mem_bare": result.metrics.get("mem_bare", None),
                    "mem_guarded": result.metrics.get("mem_guarded", None),
                    "guard_overhead_mem": result.metrics.get(
                        "guard_overhead_mem", None
                    ),
                    "rmt_outliers_bare": result.metrics.get("rmt_outliers_bare", None),
                    "rmt_outliers_guarded": result.metrics.get(
                        "rmt_outliers_guarded", None
                    ),
                    "tying_violations_post": result.metrics.get(
                        "tying_violations_post", None
                    ),
                    "epsilon": result.epsilon_used,
                    "pass": result.gates,
                }
            )
        else:
            # Skipped scenario
            scenario_data.update(
                {
                    "primary_metric_bare": None,
                    "primary_metric_guarded": None,
                    "primary_metric_overhead": None,
                    "latency_bare": None,
                    "latency_guarded": None,
                    "guard_overhead_time": None,
                    "mem_bare": None,
                    "mem_guarded": None,
                    "guard_overhead_mem": None,
                    "rmt_outliers_bare": None,
                    "rmt_outliers_guarded": None,
                    "tying_violations_post": None,
                    "epsilon": None,
                    "pass": {
                        "spike": None,
                        "tying": None,
                        "rmt": None,
                        "quality": None,
                        "time": None,
                        "mem": None,
                    },
                }
            )

        scenarios_data.append(scenario_data)

    return {
        "schema_version": summary.schema_version,
        "profile": summary.config.profile,
        "seed": summary.config.seed,
        "epsilon": summary.config.epsilon,
        "scenarios": scenarios_data,
    }


def _generate_step14_markdown(summary: BenchmarkSummary) -> str:
    """Generate Step 14 compliant Markdown report."""
    lines = [
        "# InvarLock Guard Effect Benchmark Report",
        "",
        f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}",
        f"**Profile:** {summary.config.profile}",
        f"**Seed:** {summary.config.seed}",
        f"**Epsilon:** {summary.config.epsilon if summary.config.epsilon is not None else 'auto'}",
        f"**Execution Time:** {summary.execution_time_seconds:.1f}s",
        f"**Overall Result:** {'‚úÖ PASS' if summary.overall_pass else '‚ùå FAIL'}",
        "",
        "## Scenario Results",
        "",
        "| Edit | Tier | Probes | Status | PM Œî | Time Œî | Mem Œî | RMT | Gates |",
        "|------|------|--------|--------|-------|--------|-------|-----|-------|",
    ]

    for result in summary.scenarios:
        if result.skipped:
            status = "‚è∏Ô∏è SKIP"
            ppl_delta = "-"
            time_delta = "-"
            mem_delta = "-"
            rmt_info = "-"
            gates_info = f"SKIP: {result.skip_reason}"
        else:
            # Determine status
            all_pass = all(result.gates.values()) if result.gates else False
            status = "‚úÖ PASS" if all_pass else "‚ùå FAIL"

            # Format metrics
            pm_overhead = result.metrics.get("primary_metric_overhead")
            if pm_overhead is not None and not math.isnan(pm_overhead):
                ppl_delta = f"{pm_overhead:+.1%}"
                if pm_overhead > 0.01:  # > 1%
                    ppl_delta = f"üî¥ {ppl_delta}"
                else:
                    ppl_delta = f"üü¢ {ppl_delta}"
            else:
                ppl_delta = "-"

            time_overhead = result.metrics.get("guard_overhead_time")
            if time_overhead is not None and not math.isnan(time_overhead):
                time_delta = f"{time_overhead:+.1%}"
                if time_overhead > 0.15:  # > 15%
                    time_delta = f"üî¥ {time_delta}"
                else:
                    time_delta = f"üü¢ {time_delta}"
            else:
                time_delta = "-"

            mem_overhead = result.metrics.get("guard_overhead_mem")
            if mem_overhead is not None and not math.isnan(mem_overhead):
                mem_delta = f"{mem_overhead:+.1%}"
                if mem_overhead > 0.10:  # > 10%
                    mem_delta = f"üî¥ {mem_delta}"
                else:
                    mem_delta = f"üü¢ {mem_delta}"
            else:
                mem_delta = "-"

            bare_outliers = result.metrics.get("rmt_outliers_bare", 0)
            guarded_outliers = result.metrics.get("rmt_outliers_guarded", 0)
            rmt_info = f"{bare_outliers}‚Üí{guarded_outliers}"

            # Gates summary
            gates_status = []
            if result.gates.get("spike", True):
                gates_status.append("üìà")
            else:
                gates_status.append("‚ùåüìà")

            if result.gates.get("rmt", True):
                gates_status.append("üî¨")
            else:
                gates_status.append("‚ùåüî¨")

            if result.gates.get("quality", True):
                gates_status.append("üìä")
            else:
                gates_status.append("‚ùåüìä")

            gates_info = " ".join(gates_status)

        lines.append(
            f"| {result.config.edit} | {result.config.tier} | {result.config.probes} | {status} | {ppl_delta} | {time_delta} | {mem_delta} | {rmt_info} | {gates_info} |"
        )

    lines.extend(
        [
            "",
            "## Legend",
            "",
            "- üü¢ Within threshold",
            "- üî¥ Exceeds threshold",
            "- üìà Spike gate",
            "- üî¨ RMT gate",
            "- üìä Quality gate",
            "",
        ]
    )

    return "\n".join(lines)


def _scenario_result_to_dict(result: ScenarioResult) -> dict[str, Any]:
    """Convert ScenarioResult to dictionary."""
    return {
        "edit": result.config.edit,
        "tier": result.config.tier,
        "probes": result.config.probes,
        "probes_used": result.probes_used,
        "skipped": result.skipped,
        "skip_reason": result.skip_reason,
        "metrics": result.metrics,
        "gates": result.gates,
        "epsilon_used": result.epsilon_used,
        "bare_success": result.bare_result.success if result.bare_result else False,
        "guarded_success": result.guarded_result.success
        if result.guarded_result
        else False,
    }


def _config_to_dict(config: BenchmarkConfig) -> dict[str, Any]:
    """Convert BenchmarkConfig to dictionary."""
    return {
        "edits": config.edits,
        "tiers": config.tiers,
        "probes": config.probes,
        "profile": config.profile,
        "dataset": config.dataset,
        "model_id": config.model_id,
        "adapter": config.adapter,
        "device": config.device,
        "seq_len": config.seq_len,
        "stride": config.stride,
        "seed": config.seed,
        "epsilon": config.epsilon,
        "strict": config.strict,
        "ppl_overhead_threshold": config.ppl_overhead_threshold,
        "guard_overhead_time_threshold": config.guard_overhead_time_threshold,
        "guard_overhead_mem_threshold": config.guard_overhead_mem_threshold,
        "catastrophic_spike_threshold": config.catastrophic_spike_threshold,
    }


def main():
    """CLI entry point for Step 14 specification."""
    parser = argparse.ArgumentParser(
        description="InvarLock Guard Effect Benchmark - Step 14",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Required arguments
    parser.add_argument(
        "--edits",
        required=True,
        help="Comma-separated list of edit types (quant_rtn)",
    )
    parser.add_argument(
        "--tiers",
        default="balanced",
        help="Comma-separated list of tiers (conservative,balanced,aggressive)",
    )
    parser.add_argument(
        "--probes", default="0", help="Comma-separated list of probe counts (0,2,4)"
    )
    parser.add_argument(
        "--profile",
        default="ci",
        choices=["ci", "release"],
        help="Benchmark profile (ci=50/50 windows, release=100/100 windows)",
    )

    # Optional threshold configuration
    parser.add_argument(
        "--epsilon",
        type=float,
        help="RMT outliers epsilon threshold (default: use resolved RMT deadband)",
    )
    parser.add_argument(
        "--strict", action="store_true", help="Set epsilon=0 (overrides --epsilon)"
    )

    # Model and dataset configuration
    parser.add_argument(
        "--dataset", default="wikitext2", help="Dataset to use for benchmarking"
    )
    parser.add_argument("--model-id", default="gpt2", help="Model identifier")
    parser.add_argument("--adapter", default="hf_gpt2", help="Model adapter to use")
    parser.add_argument(
        "--device", default="auto", help="Device to use (auto|cuda|mps|cpu)"
    )
    parser.add_argument(
        "--seq-len", type=int, default=512, help="Sequence length for tokenization"
    )
    parser.add_argument(
        "--stride", type=int, default=128, help="Stride for window generation"
    )
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--out", default="benchmarks", help="Output directory")
    parser.add_argument(
        "--verbose", "-v", action="store_true", help="Enable verbose logging"
    )

    args = parser.parse_args()

    # Configure logging
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    # Parse lists
    edits = [edit.strip() for edit in args.edits.split(",")]
    tiers = [tier.strip() for tier in args.tiers.split(",")]
    probes = [int(probe.strip()) for probe in args.probes.split(",")]

    # Validate inputs ‚Äî only quant_rtn is supported
    valid_edits = {"quant_rtn"}
    valid_tiers = {"conservative", "balanced", "aggressive"}

    for edit in edits:
        if edit not in valid_edits:
            print(
                f"‚ùå Invalid edit type: {edit}. Valid: {', '.join(sorted(valid_edits))}"
            )
            sys.exit(1)

    for tier in tiers:
        if tier not in valid_tiers:
            print(f"‚ùå Invalid tier: {tier}. Valid: {', '.join(sorted(valid_tiers))}")
            sys.exit(1)

    for probe in probes:
        if probe < 0:
            print(f"‚ùå Invalid probe count: {probe}. Must be >= 0")
            sys.exit(1)

    # Prepare kwargs
    kwargs = {
        "dataset": args.dataset,
        "model_id": args.model_id,
        "adapter": args.adapter,
        "device": args.device,
        "seq_len": args.seq_len,
        "stride": args.stride,
        "seed": args.seed,
    }

    try:
        # Run benchmark
        result = run_guard_effect_benchmark(
            edits=edits,
            tiers=tiers,
            probes=probes,
            profile=args.profile,
            output_dir=args.out,
            epsilon=args.epsilon,
            strict=args.strict,
            **kwargs,
        )

        # Exit with appropriate code per Step 14 specification
        if result["overall_pass"]:
            print("‚úÖ All gates passed!")
            sys.exit(0)
        else:
            print("‚ùå Some gates failed!")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n‚ùå Benchmark interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Benchmark failed: {e}")
        if args.verbose:
            import traceback

            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
